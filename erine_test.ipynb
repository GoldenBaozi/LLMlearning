{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ERINE for text classification\n",
    "\n",
    "created by [JJC@RUC](mailto:jincheng_jiang@foxmail.com)\n",
    "\n",
    "This notebook follows [the official tutorial](https://github.com/PaddlePaddle/PaddleHub/blob/release/v2.0.0-beta/demo/text_classification/README.md) and [customized dataset tutorial](https://paddlehub.readthedocs.io/zh-cn/release-v2.1/finetune/customized_dataset.html#id11), trying to mimic the work of [Jin et al. 2024](https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFDAUTO&filename=jjyj202403005) at some level\n",
    "\n",
    "To avoid the trouble of environment set-up and GPU, you can run this on a remote sever such as `matgo.cn` (not promoting!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make sure you have paddle and paddlehub installed\n",
    "## if anythong went wrong, you can run `pip install -U paddlepaddle` from your shell\n",
    "import paddlehub as hub\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import csv\n",
    "# import os\n",
    "# import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-06-01 12:14:24,238] [    INFO]\u001b[0m - Loading weights file from cache at /root/.paddlenlp/models/ernie-tiny/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2024-06-01 12:14:24,714] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "\u001b[32m[2024-06-01 12:14:31,469] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.\n",
      "\u001b[0m\n",
      "\u001b[33m[2024-06-01 12:14:31,471] [ WARNING]\u001b[0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.paddlehub/modules/ernie_tiny\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ErnieTiny(\n",
       "  (model): ErnieForSequenceClassification(\n",
       "    (ernie): ErnieModel(\n",
       "      (embeddings): ErnieEmbeddings(\n",
       "        (word_embeddings): Embedding(50006, 1024, padding_idx=0, sparse=False)\n",
       "        (position_embeddings): Embedding(600, 1024, sparse=False)\n",
       "        (token_type_embeddings): Embedding(2, 1024, sparse=False)\n",
       "        (layer_norm): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "        (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): LayerList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=4096, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=4096, out_features=1024, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[1024], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): ErniePooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, dtype=float32)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "    (classifier): Linear(in_features=1024, out_features=2, dtype=float32)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prepare the `ernie_tiny` model and inspect its properties\n",
    "## if you load the model for the first time, it will be automatically downloaded to `/root/.paddlehub/modules/ernie_tiny` on unix-like system\n",
    "## you can always specify the place for the downloaded directory, only if you remember it for later reference\n",
    "testmodel = hub.Module(name='ernie_tiny', version='2.0.1', task='seq-cls', num_classes=2) # load model\n",
    "print(testmodel.directory) # print directory of the model\n",
    "testmodel # show structure of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear data\n",
    "## mydata has two labels, about 180000 obs\n",
    "## 8:1:1 -> train:dev:test\n",
    "rawdata = pd.read_csv(\"/mnt/data/data.csv\")\n",
    "rawdata = rawdata[['label','text']]\n",
    "rawdata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my label ratio is balanced so just using `sample` to split.\n",
    "train = rawdata.sample(frac=0.8, random_state=0)\n",
    "others = rawdata.drop(train.index)\n",
    "test = others.sample(frac=0.5, random_state=0)\n",
    "dev = others.drop(test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/mnt/ERINE_proj/data'\n",
    "train.to_csv(path+'/train.csv', sep='\\t', index=False, header=True)\n",
    "dev.to_csv(path+'/dev.csv', sep='\\t', index=False, header=True)\n",
    "test.to_csv(path+'/test.csv', sep='\\t', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare dataset\n",
    "## as the official tutor said, the data can be saved in txt or csv file\n",
    "## with first column label, second column text, delimited by Tab (that is '\\t')\n",
    "## it is recommended that you save your data in the folder 'your project name'/data\n",
    "from paddlehub.datasets.base_nlp_dataset import TextClassificationDataset\n",
    "\n",
    "class MyDataset(TextClassificationDataset):\n",
    "    # \n",
    "    base_path = '/mnt/ERINE_proj/data'\n",
    "    # label list\n",
    "    label_list=['data', 'nondata']\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_len: int = 128, mode: str = 'train'):\n",
    "        if mode == 'train':\n",
    "            data_file = 'train.csv'\n",
    "        elif mode == 'test':\n",
    "            data_file = 'test.csv'\n",
    "        else:\n",
    "            data_file = 'dev.csv'\n",
    "        super().__init__(\n",
    "            base_path=self.base_path,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "            mode=mode,\n",
    "            data_file=data_file,\n",
    "            label_list=self.label_list,\n",
    "            is_file_with_header=True)\n",
    "\n",
    "\n",
    "tokenizer = testmodel.get_tokenizer()\n",
    "train_dataset = MyDataset(tokenizer)\n",
    "dev_dataset = MyDataset(tokenizer=tokenizer, mode='dev')\n",
    "test_dataset = MyDataset(tokenizer=tokenizer, mode='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model (fine-tuning)\n",
    "## my dataset is rather large, so training is time-consuming(ETA 194 mins) \n",
    "## so I interupt from keyboard\n",
    "## but it takes only 30/4568 batches in the first epoch for acc to increase from 0.68 to 0.83\n",
    "## which is surprising.\n",
    "import paddle\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=5e-5, parameters=testmodel.parameters()) # use Adam optimizer\n",
    "trainer = hub.Trainer(testmodel, optimizer, checkpoint_dir='/mnt/ERINE_proj/test_ernie_text_cls') # setup trainer\n",
    "trainer.train(train_dataset, epochs=10, batch_size=32, eval_dataset=dev_dataset, num_workers=8) # start training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap to Jin et al. 2024\n",
    "\n",
    "- may be we need a smaller training set, or larger batch size, or better GPU to speed up training\n",
    "- 8-label classification\n",
    "- smaller training units (setences instead of paragraphs)\n",
    "- comparison with other models, using more metrics (recall, precision, F1, F.8), using `sklearn.metrics`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
